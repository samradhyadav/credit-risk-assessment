# ðŸ’³ Credit Risk Assessment using Probability, Statistics & Machine Learning  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/samradhyadav/Credit-Risk-Assessment/blob/main/CreditRiskUsingProbability%26Statistics.ipynb)
![Python](https://img.shields.io/badge/Python-3.10-yellow.svg)
![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)



Welcome to my deep dive into **Credit Risk Assessment** using real-world data!  
This project combines **Probability & Statistics** with **Machine Learning** to predict the likelihood of loan defaults, empowering better financial decisions and risk management.

---

## ðŸŽ¯ Project Idea & Motivation

In today's digital financial landscape, lending institutions are increasingly relying on **data-driven strategies** to evaluate the creditworthiness of borrowers. Traditional credit scoring methods, while useful, often fall short in capturing the nuanced patterns hidden in large-scale financial data.

This project was born out of the idea to **leverage probability and statistics**, along with **machine learning**, to build a more robust and interpretable system for **Credit Risk Assessment**. 

---

### ðŸ’¡ Why This Project?

- ðŸ’³ **Real-World Relevance**:  
  Credit risk evaluation lies at the heart of banking and lending institutions. With the rise in peer-to-peer lending platforms like LendingClub, there's a massive volume of structured loan data available to analyze and improve loan decisioning systems.

- ðŸ“ˆ **Data Science Meets Finance**:  
  This project was a perfect opportunity to apply **statistical analysis**, **feature engineering**, and **modeling** techniques to a domain that affects millionsâ€”finance and lending.

- ðŸ” **Explainability and Insight**:  
  Unlike black-box models, I emphasize **explainable patterns**. Using probability distributions, bivariate plots, and box plots, I explore the **underlying reasons** behind loan defaults â€” not just predictions.

- ðŸŽ“ **Learning Goals**:
    - Apply statistical thinking to real-world data  
    - Visualize trends that affect loan outcomes  
    - Understand which variables truly matter in predicting defaults  
    - Practice end-to-end machine learning workflow in a business-critical domain

---

## ðŸ“ Project Overview

**Goal**:  
Predict whether a loan will default based on historical data using statistical insights and ML models.

---

## ðŸ“Š Exploratory Data Analysis (EDA)

- Histograms, Box Plots: Understand distributions and outliers
- Violin & Scatter Plots: Relationship between `interest rate`, `FICO scores`, `installments`, and `loan status`
- Mutual Information & Random Forest Importances: Used to identify top features

---

## ðŸ§  Modeling Approach

I tested and evaluated two strong classifiers:

| Model               | AUC Score |
|--------------------|-----------|
| Logistic Regression | 0.89      |
| Random Forest       | 0.89      |

### âœ… Techniques Used

- **Feature Selection**: via Random Forest importance & Mutual Information
- **Evaluation**: 
  - ROC Curve
  - AUC Score
  - **Confusion Matrix** for classification quality
  - **Cross-validation** for stable performance estimates

---

## ðŸ“Œ Key Findings

- ðŸ”º **Higher interest rates** are significantly correlated with **loan defaults**
- ðŸ”» **Lower FICO scores** (both low and high bounds) strongly indicate a higher risk of default
- ðŸ§¾ Features like `recoveries` and `collection_recovery_fee` are skewed and strongly associated with defaulted loans
- âž– `Installment` amount is a weak predictor of default
- ðŸ“Š Mutual Information & Random Forest consistently rank `int_rate`, `fico_range_low`, `recoveries` as top features

---

## ðŸ§® How Probability, Statistics & ML Worked Together

- **Probability**: Helped estimate event likelihoods (default vs non-default) from historical data patterns
- **Statistics**: Used for understanding distributions (mean, median, skewness, outliers) and validating assumptions
- **ML Models**: Took these insights to **learn complex patterns**, provide **accurate predictions**, and **rank features** by importance

This synergy between explainable statistics and predictive modeling made the approach both interpretable and accurate.

---

## ðŸ“‚ Files

| File / Folder | Description |
|---------------|-------------|
| `CreditRiskUsingProbability&Statistics.ipynb` | Full notebook with EDA, modeling, and evaluation |
| `README.md` | Project documentation |
| `LICENSE` | Open-source license (MIT recommended) |
| `requirements.txt` | List of required Python packages |
| `dataset` | Contains data and data source link |


---

## ðŸ“š Dataset Used

- [Give Me Some Credit â€“ Kaggle Competition](https://www.kaggle.com/competitions/GiveMeSomeCredit/data)

---

## âš™ï¸ Tech Stack

- ðŸ Python 3
- ðŸ“Š Pandas, NumPy, Matplotlib, Seaborn
- ðŸ” Scikit-learn
- ðŸš€ Google Colab (for interactive development)

---

## ðŸ§  Inspired By

The need to quantify creditworthiness using data-driven techniques rather than gut feeling or rigid rules.

---

## ðŸš€ Future Work

- Hyperparameter tuning with GridSearchCV
- Ensemble techniques (e.g. XGBoost, LightGBM)
- Handling class imbalance (SMOTE)
- Deployment via Flask or Streamlit

---

## ðŸ“¬ Connect with Me

ðŸ“§ [reach.samradhyadav@gmail.com]  
ðŸ’¼ [LinkedIn](https://www.linkedin.com/in/samradh-singh-yadav/)  
ðŸ“Œ Star this repo if you like it!

---

> _"Data beats emotions." â€” Sean Rad_

---

## ðŸ“„ License

This project is licensed under the terms of the [MIT License](LICENSE).
